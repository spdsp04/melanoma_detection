{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Durgesh_Chaubey_NN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP5AgyroIUZnvi3vgN/U2EE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spdsp04/melanoma_detection_Durgesh_Chaubey_CNN/blob/main/Durgesh_Chaubey_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem Statement (Melanoma Detection)\n",
        "\n",
        ">To build a CNN based model which can accurately detect melanoma. Melanoma is a type of cancer that can be deadly if not detected early. It accounts for 75% of skin cancer deaths. A solution that can evaluate images and alert dermatologists about the presence of melanoma has the potential to reduce a lot of manual effort needed in diagnosis.\n",
        "\n",
        ">The dataset consists of 2357 images of malignant and benign oncological diseases, which were formed from the International Skin Imaging Collaboration (ISIC). All images were sorted according to the classification taken with ISIC, and all subsets were divided into the same number of images, with the exception of melanomas and moles, whose images are slightly dominant."
      ],
      "metadata": {
        "id": "iuejXPJqKmVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Installing keras\n",
        "!pip install q keras==1.2.2"
      ],
      "metadata": {
        "id": "AdpU32yAHwm7",
        "outputId": "e8fc3010-8d8b-4836-c99d-8981538936aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: q in /usr/local/lib/python3.7/dist-packages (2.6)\n",
            "Collecting keras==1.2.2\n",
            "  Using cached Keras-1.2.2-py3-none-any.whl\n",
            "Requirement already satisfied: theano in /usr/local/lib/python3.7/dist-packages (from keras==1.2.2) (1.0.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==1.2.2) (3.13)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from keras==1.2.2) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from theano->keras==1.2.2) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from theano->keras==1.2.2) (1.4.1)\n",
            "Installing collected packages: keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: Keras 2.3.1\n",
            "    Uninstalling Keras-2.3.1:\n",
            "      Successfully uninstalled Keras-2.3.1\n",
            "Successfully installed keras-1.2.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# confirming installation of keras and Checking version of Kears \n",
        "%tensorflow_version 1.x\n",
        "import keras\n",
        "keras.__version__"
      ],
      "metadata": {
        "id": "bR7A59vaO3j4",
        "outputId": "2346dbe2-a3eb-4cd4-806c-6cb8e384673a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.3.1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6THCYX0twCIN"
      },
      "outputs": [],
      "source": [
        "# Importing Libraries\n",
        "\n",
        "import pathlib\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import PIL\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "#from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization, Rescaling, InputLayer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Dataset from Google Drive"
      ],
      "metadata": {
        "id": "sGoavEARKO6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "AbR2xo_OJBZc",
        "outputId": "7ed93e27-722a-47c2-c1cc-d7d001847986",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This assignment uses a dataset of about 2357 images of skin cancer types. The dataset contains 9 sub-directories in each train and test subdirectories. The 9 sub-directories contains the images of 9 skin cancer types respectively."
      ],
      "metadata": {
        "id": "I500YxmHRMi_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the path for train and test images\n",
        "## Todo: Update the paths of the train and test dataset\n",
        "data_dir_train = pathlib.Path(\"/content/drive/MyDrive/Skin cancer ISIC The International Skin Imaging Collaboration/Train\")\n",
        "data_dir_test = pathlib.Path('/content/drive/MyDrive/Skin cancer ISIC The International Skin Imaging Collaboration/Test')"
      ],
      "metadata": {
        "id": "EIKVM9pDH15q"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_count_train = len(list(data_dir_train.glob('*/*.jpg')))\n",
        "print(image_count_train)\n",
        "image_count_test = len(list(data_dir_test.glob('*/*.jpg')))\n",
        "print(image_count_test)"
      ],
      "metadata": {
        "id": "V9xrh3gxwE9i",
        "outputId": "83523fee-dfd9-41da-f047-610156a6b2d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2239\n",
            "118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Image checking with keras.preprocessing tool**\n",
        "\n",
        "Let's use the image_dataset_from_directory utility to check data.\n",
        "\n",
        "**Setting up dataset**\n",
        "\n",
        "finallising size of data for loader"
      ],
      "metadata": {
        "id": "WXCEAnPY_gX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "img_height = 180\n",
        "img_width = 180"
      ],
      "metadata": {
        "id": "SN3xtFbNALZd"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classifying train (70%) and test data (30%)\n",
        "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "  data_dir_train,\n",
        "  label_mode=\"int\",\n",
        "  validation_split=0.3,\n",
        "  subset=\"training\",\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)"
      ],
      "metadata": {
        "id": "JqrN_uzrAkrY",
        "outputId": "dde9e42b-11f4-4920-933e-6dba2a4b3b1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-b2b2eeb279c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Classifying train (70%) and test data (30%)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m train_dataset = tf.keras.utils.image_dataset_from_directory(\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mdata_dir_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mlabel_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"int\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/util/module_wrapper.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    191\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m       \u001b[0mattr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfmw_wrapped_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfmw_public_apis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.python.keras.api._v1.keras.utils' has no attribute 'image_dataset_from_directory'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Classifying test dataset\n",
        "test_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "  data_dir_train,\n",
        "  label_mode=\"int\",\n",
        "  validation_split=0.3,\n",
        "  subset=\"validation\",\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)"
      ],
      "metadata": {
        "id": "vBT6u7tcBR8v",
        "outputId": "69ea5a23-4cc3-4461-f055-0b85e31edb51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-60f359a8ee3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Classifying test dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m test_dataset = tf.keras.utils.image_dataset_from_directory(\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mdata_dir_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mlabel_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"int\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/util/module_wrapper.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    191\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m       \u001b[0mattr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfmw_wrapped_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfmw_public_apis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.python.keras.api._v1.keras.utils' has no attribute 'image_dataset_from_directory'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Listing out all the classes of skin cancer\n",
        "class_names = train_ds.class_names\n",
        "print(class_names)"
      ],
      "metadata": {
        "id": "-CCvyqVoBlPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Listing out train images in each classes\n",
        "for i in class_names:\n",
        "  print(i,\": \",len(list(data_dir_train.glob('{}/*.jpg'.format(i)))))"
      ],
      "metadata": {
        "id": "391MfvOJCBWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Listing out test images in each classes\n",
        "for i in class_names:\n",
        "  print(i,\": \",len(list(data_dir_test.glob('{}/*.jpg'.format(i)))))"
      ],
      "metadata": {
        "id": "9cg8B_8JCBZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Visulisation   \n",
        "\n",
        "Visualizing one instance of all the nine classes present in the dataset"
      ],
      "metadata": {
        "id": "dnnVcZP0CKks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10,10))\n",
        "for index,images in enumerate(class_names):\n",
        "  \n",
        "  temp=plt.imread(list(data_dir_train.glob('{}/*.jpg'.format(images)))[0])\n",
        "  \n",
        "  plt.subplot(3,3,index+1)\n",
        "  plt.imshow(temp)\n",
        "  plt.axis('off')\n",
        "  plt.title(images)"
      ],
      "metadata": {
        "id": "O2O8XogsCBc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking shape of image\n",
        "for image_batch, labels_batch in train_ds:\n",
        "  print(image_batch.shape)\n",
        "  print(labels_batch.shape)\n",
        "  break"
      ],
      "metadata": {
        "id": "j-qZ8vAECBfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The image_batch is a tensor of the shape (32, 180, 180, 3). This is a batch of 32 images of shape 180x180x3 (the last dimension refers to color channels RGB). The label_batch is a tensor of the shape (32,), these are corresponding labels to the 32 images.\n",
        "\n",
        "Dataset.cache() keeps the images in memory after they're loaded off disk during the first epoch.\n",
        "\n",
        "Dataset.prefetch() overlaps data preprocessing and model execution while training."
      ],
      "metadata": {
        "id": "ZB8BvE1JCho1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "KKsMl8bmCBie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standardize the data\n",
        "\n",
        "The RGB channel values are in the [0, 255] range. This is not ideal for a neural network; in general we should seek to make our input values small.\n",
        "\n",
        "Standardize values should to be in the [0, 1] range."
      ],
      "metadata": {
        "id": "QmK2RkgKClcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
        "normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "image_batch, labels_batch = next(iter(normalized_ds))"
      ],
      "metadata": {
        "id": "oiowzj1RCBlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the model (Model 1)\n",
        "\n",
        "Creating a CNN model, which can accurately detect 9 classes present in the dataset."
      ],
      "metadata": {
        "id": "XYfvYWXmCsCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model=Sequential([layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width,3))])\n",
        "\n",
        "model.add(Conv2D(16, 3, activation='relu',padding='same', input_shape = (180, 180, 32)))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(32, 3, activation='relu',padding='same'))\n",
        "model.add(MaxPooling2D())\n",
        "model.add(Conv2D(64, 3, activation='relu',padding='same'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(9, activation='softmax'))"
      ],
      "metadata": {
        "id": "6NJECfiaCBoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Compile the model\n",
        "\n",
        "Choosing the following appropirate optimiser and loss function for model training"
      ],
      "metadata": {
        "id": "swOxELP9Cx1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "9cTiQibICBrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View the summary of all layers\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "E6yvsVKKC4AT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model"
      ],
      "metadata": {
        "id": "mueVF9RWC-97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "history = model.fit(\n",
        "  train_ds,\n",
        "  validation_data=val_ds,\n",
        "  epochs=epochs\n",
        ")"
      ],
      "metadata": {
        "id": "0NVccHLIC4C7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizing training results"
      ],
      "metadata": {
        "id": "TbqWqMUADEXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uSv-QOWvC4F9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Result:\n",
        "\n",
        "Train Accuracy - 0.88\n",
        "\n",
        "Train Loss - 0.29\n",
        "\n",
        "Validation Accuracy - 0.56\n",
        "\n",
        "Validation Loss - 2.11\n",
        "\n",
        "Findings:\n",
        "\n",
        "Clearly by comparing the above results we can say that the model is overfitting as we have high Train Accuracy and Low Validation Accuracy. Also Loss value is higher in validation dataset than train dataset.\n",
        "\n",
        "Solution:\n",
        "\n",
        "Need to do some changes in model like doing data augmentation, including dropout in layers"
      ],
      "metadata": {
        "id": "RD0i4zRmDNek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Augmentation**\n",
        "\n",
        "Applying Data Augmentation technique like Flip, Rotate, Zoom for input dataset then we can build the model and check for results. "
      ],
      "metadata": {
        "id": "P4S_N1BzDRDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_aug = keras.Sequential([layers.experimental.preprocessing.RandomFlip(mode=\"horizontal_and_vertical\",input_shape=(img_height,img_width,3)),\n",
        "                             layers.experimental.preprocessing.RandomRotation(0.2, fill_mode='reflect'),\n",
        "                             layers.experimental.preprocessing.RandomZoom(height_factor=(0.2, 0.3), width_factor=(0.2, 0.3), fill_mode='reflect')])"
      ],
      "metadata": {
        "id": "3kjbJr60C4Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "image_aug1=ImageDataGenerator(rescale=1./255,\n",
        "                   horizontal_flip=True,\n",
        "                   zoom_range=0.2)\n",
        "image_batch,label_batch=next(iter(train_ds))\n",
        "temp=image_batch[0].numpy()\n",
        "plt.imshow(temp.astype('uint8'))"
      ],
      "metadata": {
        "id": "iIlWYlo1C4LL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualize how your augmentation strategy works for one instance of training image.\n",
        "plt.imshow(image_aug1.apply_transform(temp,transform_parameters={'flip_horizontal':True}).astype('uint8'))"
      ],
      "metadata": {
        "id": "2nsrSBhiC4Ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 2:"
      ],
      "metadata": {
        "id": "BDPPMaOQDeTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model\n",
        "model1=Sequential([image_aug,\n",
        "                    layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width,3))\n",
        "      \n",
        "])\n",
        "model1.add(Conv2D(16, 3, activation='relu',padding='same',input_shape = (180, 180, 32)))\n",
        "model1.add(MaxPooling2D())\n",
        "\n",
        "model1.add(Conv2D(32, 3, activation='relu',padding='same'))\n",
        "model1.add(MaxPooling2D())\n",
        "model1.add(Conv2D(64, 3, activation='relu',padding='same'))\n",
        "model1.add(MaxPooling2D())\n",
        "model1.add(Dropout(0.25))\n",
        "\n",
        "model1.add(Flatten())\n",
        "model1.add(Dense(128, activation='relu'))\n",
        "model1.add(Dense(9, activation='softmax'))"
      ],
      "metadata": {
        "id": "FhGn0AhLC4Qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model1.compile(optimizer='adam',\n",
        "              loss=tf.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "us3tARPaC4TC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View the summary of all layers\n",
        "model1.summary()"
      ],
      "metadata": {
        "id": "L-RmzhKMC4V4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "epochs = 20\n",
        "history = model1.fit(\n",
        "  train_ds,\n",
        "  validation_data=val_ds,\n",
        "  epochs=epochs\n",
        ")"
      ],
      "metadata": {
        "id": "JgGKTxY5DlFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the result\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3iYAEsTtDlHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result:**\n",
        "\n",
        "Train Accuracy - 0.58\n",
        "\n",
        "Validation Accuracy - 0.54\n",
        "\n",
        "Train Loss - 1.13\n",
        "\n",
        "Validation Loss - 1.32\n",
        "\n",
        "**Findings:**\n",
        "\n",
        "From the above results we can see that model is underfitting. Eventhough Train and validation accuracy is almost near but the value is less we got only 55% which is not good accuracy. \n",
        "\n",
        "**Solution:**\n",
        "\n",
        "We can check for class imbalance and rectify using Augmentor package, can do batch normalization, dropout in layers. "
      ],
      "metadata": {
        "id": "iZpXPWurD0vC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking for class imbalance:**"
      ],
      "metadata": {
        "id": "FJBfVhyoD4JD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Many datasets can have class imbalance, one class can have proportionately higher number of samples compared to the others. Class imbalance can have a detrimental effect on the final model quality. Hence as a sanity check it becomes important to check what is the distribution of classes in the data."
      ],
      "metadata": {
        "id": "mr2AnQSCD5FK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in class_names:\n",
        "  print(i,\": \",len(list(data_dir_train.glob('{}/*.jpg'.format(i)))))"
      ],
      "metadata": {
        "id": "BLK_59q0DlKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize class imbalance through chart\n",
        "count=[]\n",
        "for i in class_names:\n",
        "    count.append(len(list(data_dir_train.glob(i+'/*.jpg'))))\n",
        "plt.figure(figsize=(25,10))\n",
        "plt.bar(class_names,count)"
      ],
      "metadata": {
        "id": "3qAxw3iUDlNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "**'seborrheic keratosis'** has lowest number of image with 77\n",
        "\n",
        "**'pigmented benign keratosis'** has more dominent(higher) number of image with 462 "
      ],
      "metadata": {
        "id": "GHEAY5WkDlQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking label\n",
        "path_list=[]\n",
        "lesion_list=[]\n",
        "for i in class_names:\n",
        "      for j in data_dir_train.glob(i+'/*.jpg'):\n",
        "        path_list.append(str(j))\n",
        "        lesion_list.append(i)\n",
        "dataframe_dict_original = dict(zip(path_list, lesion_list))\n",
        "original_df = pd.DataFrame(list(dataframe_dict_original.items()),columns = ['Path','Label'])\n",
        "original_df"
      ],
      "metadata": {
        "id": "ES4LysI4DlTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "**Augmentor Package**\n",
        "\n",
        "A python package known as Augmentor is used to add more samples across all classes so that none of the classes have very few samples."
      ],
      "metadata": {
        "id": "6rpSVBrADlV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Augmentor\n",
        "!pip install Augmentor"
      ],
      "metadata": {
        "id": "nkaV-wbCDlYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import Augmentor\n",
        "for i in class_names:\n",
        "  augmnt_pipeline = Augmentor.Pipeline(str(data_dir_train) + '/'+ i)\n",
        "  augmnt_pipeline.rotate(probability=0.7, max_left_rotation=10, max_right_rotation=10)\n",
        "  augmnt_pipeline.sample(500)"
      ],
      "metadata": {
        "id": "z5jSqthdDlad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Augmentor has stored the augmented images in the output sub-directory of each of the sub-directories of skin cancer types.. Lets take a look at total count of augmented images."
      ],
      "metadata": {
        "id": "2bFwIZzrEO9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_count_train = len(list(data_dir_train.glob('*/output/*.jpg')))\n",
        "print(image_count_train)"
      ],
      "metadata": {
        "id": "gvb_obyzELLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking distribution of augmented data after adding new images to the original training data.**"
      ],
      "metadata": {
        "id": "7JGEfiwyEU7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "from glob import glob\n",
        "path_list_new = [x for x in glob(os.path.join(data_dir_train, '*','output', '*.jpg'))]"
      ],
      "metadata": {
        "id": "BGipnxxWELOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lesion_list_new = [os.path.basename(os.path.dirname(os.path.dirname(y))) for y in glob(os.path.join(data_dir_train, '*','output', '*.jpg'))]"
      ],
      "metadata": {
        "id": "fFg9DxZhELQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe_dict_new = dict(zip(path_list_new, lesion_list_new))"
      ],
      "metadata": {
        "id": "6Q2XrGHaELTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = pd.DataFrame(list(dataframe_dict_new.items()),columns = ['Path','Label'])\n",
        "new_df = original_df.append(df2)"
      ],
      "metadata": {
        "id": "IcgwEbMuELWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df['Label'].value_counts()"
      ],
      "metadata": {
        "id": "XqdpR4UAELY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have added 500 images to all the classes to maintain some class balance."
      ],
      "metadata": {
        "id": "_0sSrQLMEiGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "img_height = 180\n",
        "img_width = 180"
      ],
      "metadata": {
        "id": "t01EdAcRELbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating a training dataset**"
      ],
      "metadata": {
        "id": "Z8Z63WqnEn1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_data_dir = data_dir_train\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  augmented_data_dir,\n",
        "  seed=123,\n",
        "  validation_split = 0.2,\n",
        "  subset ='training',\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)"
      ],
      "metadata": {
        "id": "dLFkIKjvEk7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Creating a validation dataset"
      ],
      "metadata": {
        "id": "W4qiS4hTEk93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  augmented_data_dir,\n",
        "  seed=123,\n",
        "  validation_split = 0.2,\n",
        "  subset ='validation',\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)"
      ],
      "metadata": {
        "id": "sckiocu6ElBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 3:"
      ],
      "metadata": {
        "id": "bjV2RaEeEwto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Include BatchNormalization, Dropout\n",
        "model_norm=Sequential()\n",
        "\n",
        "model_norm.add(InputLayer((180,180,3)))\n",
        "model_norm.add(Rescaling(1./255))\n",
        "\n",
        "model_norm.add(Conv2D(32, (3, 3), padding='same'))\n",
        "model_norm.add(BatchNormalization())\n",
        "model_norm.add(Activation('relu'))\n",
        "\n",
        "model_norm.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model_norm.add(Conv2D(64, (3, 3)))\n",
        "model_norm.add(BatchNormalization())\n",
        "model_norm.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model_norm.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model_norm.add(Dropout(0.25))\n",
        "\n",
        "model_norm.add(Flatten())\n",
        "model_norm.add(Dense(512, activation='relu'))\n",
        "model_norm.add(Dense(256, activation='relu'))\n",
        "model_norm.add(Dropout(0.25))\n",
        "\n",
        "model_norm.add(Dense(9, activation='softmax'))\n",
        "\n",
        "\n",
        "# View the summary of all layers\n",
        "model_norm.summary()"
      ],
      "metadata": {
        "id": "qjwsKzEqElEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compile Model**"
      ],
      "metadata": {
        "id": "YD49wP3YE5Lo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#choose an appropirate optimiser and loss function\n",
        "model_norm.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "eM8CZn1cElHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train the model**"
      ],
      "metadata": {
        "id": "8B2Urvj_E7Uq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 30\n",
        "history = model_norm.fit(\n",
        "  train_ds,\n",
        "  validation_data=val_ds ,\n",
        "  epochs=epochs,\n",
        ")\n"
      ],
      "metadata": {
        "id": "Zdm2H9pmELd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "**Visualizing the result**"
      ],
      "metadata": {
        "id": "8sdBTx7UE89x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PaDyZbdvE9Ao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result:**\n",
        "\n",
        "Train Accuracy - 0.91\n",
        "\n",
        "Validation Accuracy - 0.79\n",
        "\n",
        "Train Loss - 0.21\n",
        "\n",
        "Validation Loss - 0.92\n",
        "\n",
        "**Findings:**\n",
        "\n",
        "From the above result we come to know this model's validation accuracy is increased when compared to previous models. But this model is also overfitting.\n",
        "\n",
        "Using class rebalance, droupout, batch normalization helps in acheiving the better result than simple model.\n",
        "\n",
        "**Solution:**\n",
        "\n",
        "The Model can be further improved by tuning the hyperparameter."
      ],
      "metadata": {
        "id": "CqQbJYxwFFtg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion:\n",
        "\n",
        "We observe successive improvement from Model 1 to Model 3:\n",
        "\n",
        "**Model 1:** Simple CNN Model \n",
        "\n",
        "Accuracy: 0.88 | Validation accuracy : 0.56\n",
        "\n",
        "**Model 2:** Data Augment with Dropout\n",
        "\n",
        "Accuracy: 0.58 | Validation accuracy : 0.54\n",
        "\n",
        "**Model 3:** Class rebalance,BatchNormalization with Dropout\n",
        "\n",
        "Accuracy: 0.91 | Validation accuracy : 0.79\n",
        "\n",
        "Accuracy can be imporved further with proper hyper-parameter. Can use different CNN Configuration, loss function, Optimizers and number of Layers and check how accuracy improves."
      ],
      "metadata": {
        "id": "ICzeSvczFJj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "55YDcQNWE9JF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KhPhH9sGE9LR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}